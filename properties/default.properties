#  Passwords should be set in private maven settings
#  They are here for reference but commented out so that the
#  assert-properties plugin will warn the user if they are not set

############################
# 
# Usernames
# 
############################
accumulo.user.name=root
mysql.user.name=sorted
#  Admin Console and JMX Console username
jboss.jmx.username=jmxadmin
#  Credentials for HORNETQ servers to join the cluster
hornetq.cluster.username=datawave-cluster
#  Your system's default username for JMS. This will be used by MDBs and other clients to connect to JMS Server. You can also create
#  additional accounts for external systems by adding entries in hornetq-users.properties and hornetq-roles.properties
hornetq.system.username=DATAWAVE

############################
# 
# Passwords
# 
############################
#server.keystore.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#accumulo.user.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#mysql.user.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#jboss.jmx.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#hornetq.cluster.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#hornetq.system.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
server.truststore.password=

############################
# 
# Server Identity
# 
############################
server.cert.basename=
# Subject DN of the server cert
server.dn=
#  Issuer DN of the server cert
issuer.dn=
server.trust.store=

############################
# 
# Security Settings
# 
############################
# Group to look under when accessing users information
security.group.name=DEFAULT
# Roles required for a user to be able to access the DATAWAVE security domain
security.required.roles=
# Name of table in which to cache users security credentials
accumulo.cache.table.name=PrincipalCache
security.use.testauthservice=false
security.testauthservice.context.entry=
security.testauthservice.entries=

############################
# 
# Server Settings
# 
############################
jboss.console.redirect=
port.definition.set=ports-default
#  JBoss Heap size, used in bin/run.conf
jboss.jvm.heap.size=4096m
#  JBoss CMSInitiatingOccupancyFraction, start a garbage collection if the tenured generation exceeds this fraction, 92% by default
jboss.cms.initiating.occupancy.fraction=75
#  Additional args for the JBoss JVM used in bin/run.conf
jboss.java.opts=
# Add no additional JAVA_OPTS to run.conf
jboss.extra.java.opts=
#  Extra stuff to append to the end of run.conf
jboss.run.conf.extras=
#location of JBoss log dir
jboss.log.hdfs.uri=hdfs://localhost:8020/
jboss.log.hdfs.dir=/WebServiceLogs
# Application Server Cluster partition name
jboss.partition.name=${env.USER}
#user the wildfly init.d script should use to run wildfly
jboss.runas.user=jboss

############################
# 
# RestEasy  Settings
# 
############################
#  Number of job result sets held in memory at once, defaults to 100
resteasy.async.job.service.max.job.results=200
#  Maximum wait time on a job when a client is querying for it, defaults to 5m in ms
resteasy.async.job.service.max.wait=300000
#  Thread pool size of background threads that run the job, defaults to 100
resteasy.async.job.service.thread.pool.size=200
#  The base path for job URIs
resteasy.async.job.service.base.path=/asynch/jobs

############################
# 
# Query Settings
# 
############################
#  Below are the table names that are put into the database.properties file in the Query package and are ultimately used
#  in the QueryLogicFactory.xml file. These are the defaults, if your table names are different then override them in
#  your profile
table.name.atom.categories=Atom
table.name.metadata=DatawaveMetadata
table.name.date.index=dateIndex
table.name.shardIndex=shardIndex
table.name.shardReverseIndex=shardReverseIndex
table.name.shard=shard
table.name.edge=edge
table.name.knowledge.metadata=knowledgeMetadata
table.name.knowledge.shardIndex=knowledgeIndex
table.name.knowledge.shardReverseIndex=knowledgeReverseIndex
table.name.knowledge.shard=knowledgeShard
table.name.error.metadata=errorMetadata
table.name.error.shardIndex=errorIndex
table.name.error.shardReverseIndex=errorReverseIndex
table.name.error.shard=errorShard
table.name.report.results.metadata=reportResultsMetadata
table.name.report.results.shardIndex=reportResultsIndex
table.name.report.results.shardReverseIndex=reportResultsReverseIndex
table.name.report.results.shard=reportResults

#  Number of minutes that a query can be idle before the connection is closed
query.expiration.minutes=15
#  Number of minutes that a query next or create call can take before it is canceled.
query.max.call.time.minutes=60
#  Number of minutes after which the page will be returned iff it contains results.  This prevents a query from being cancelled re query.max.call.time.minutes if there are results.
query.page.shortcircuit.minutes=55
#  Number of minutes after which the page size velocity will be checked (percent page full vs percent call time complete) to potentially short circuit the next call
query.page.size.shortcircuit.minutes=30
# The max page size that a user can request.
query.default.page.size=10
# The max page size that a user can request.  0 turns off this feature
query.max.page.size=10000
# The number of bytes at which a page will be returned, event if the pagesize has not been reached.  0 turns off this feature
query.page.byte.trigger=0
# Determine whether or not we collapse UIDS into a sharded range when doing the rangestream lookup
query.collapse.uids=false
# Determine when we give up on an global index scan and push down to the field index.  Default is virtually unlimited (1 year).
query.max.index.scan.ms=31536000000
# Suppresses documents which would otherwise have only index only fields within it
disable.index.only.documents=false
# Indicates whether index-only filter functions should be enabled, such as filter:includeRegex()
enable.index.only.filter.functions=false
query.tld.collapse.uids=false
# Indicates whether tuples loaded for an event should be filtered based on field and value (true) or just based on field (false)
query.tld.query.expression.filter.enabled=false
query.parent.query.expression.filter.enabled=false
query.ancestor.query.expression.filter.enabled=false

############################
# 
# Accumulo Connection Pools
# 
############################
accumulo.instance.name=accumulo
zookeeper.hosts=localhost:2181

#  Number of connections in the connection pools to the accumulo instance. If not enough connections, then operations will block
#  until a connection becomes available. Be careful here as when used in a batch scanner, a connection will use N threads and network
#  connections when querying ACCUMULO.These are the defaults, if your table names are different then override them in
#  your profile
accumulo.low.defaultpool.size=25
accumulo.normal.defaultpool.size=50
accumulo.high.defaultpool.size=100
accumulo.admin.defaultpool.size=200
accumulo.low.uuidpool.size=1
accumulo.normal.uuidpool.size=2
accumulo.high.uuidpool.size=3
accumulo.admin.uuidpool.size=5
accumulo.low.fipool.size=1
accumulo.normal.fipool.size=2
accumulo.high.fipool.size=3
accumulo.admin.fipool.size=5

############################
# 
# EJB Settings
# 
############################
#  Number of MDBs in the pool for auditing to the log file, max jboss.mdb.pool.max.size. This will determine number of concurrent writers to the log
audit.log.mdb.pool.size=20
#  Number of MDBs in the pool for auditing to ACCUMULO, max jboss.mdb.pool.max.size. This will determine number of concurrent writers to the log table
audit.cb.mdb.pool.size=50
# Audit queue names
audit.queues=LogAuditQueue,ServerLogAuditQueue,AccumuloAuditQueue
# Number of threads available for EJB3 asynchronous methods
jboss.ejb3.async.threads=10
#  Number of seconds before transactions will time out (NOTE: This should ne &gt; query.max.call.time.minutes)
jboss.transaction.time.out=3900
#  Number of ms before the remote ejb connections will time out (NOTE: This should be &gt; query.max.call.time.minutes)
jboss.ejb3.connector.time.out=3900000
#  Number of threads for accepting HTTP requests, defaults to 200
jboss.web.max.threads=200
#  Number of requests to queue up for available thread. When queue is full then connection refused errors will be returned to the caller
jboss.web.accept.count=200
#  Maximum number of Stateless Session Bean instances in each pool
jboss.slsb.pool.max.size=200
#  Timeout (ms) before throwing an exception when waiting to get a Stateless Session Bean instance from the pool
jboss.slsb.pool.timeout=30000
#  Maximum number of Message Driven Bean instances in each pool
jboss.mdb.pool.max.size=200
#  Timeout (ms) before throwing an exception when waiting to get a Message Driven Bean instance from the pool
jboss.mdb.pool.timeout=30000

############################
# 
# HornetQ Settings
# 
############################
#  HORNETQ JMS DataSource max pool size
hornetq.datasource.max.pool.size=200
hornetq.host=
hornetq.port=

############################
# 
# DATAWAVE Settings
# 
############################
#  Transport guarantee for web apps
webapp.transport.guarantee=CONFIDENTIAL
#  Tell the login module to expect client cert, and not DN stuffed in a header.
trusted.header.login=false
#  web service response namespaces
datawave.webservice.namespace=http://webservice.datawave/v1
# Name of the Cluster
cluster.name=DEV

############################
#
# Timely metrics reporting
#
############################
metrics.reporter.host=localhost
metrics.reporter.port=54321
metrics.reporter.class=datawave.metrics.TimelyMetricsReporterFactory

############################
#
# EventQuery
#
############################
#  Default set of filter properties (which are disabled) 
event.query.filters.enabled=false
event.query.filters.classnames=
event.query.filters.options=
event.query.filters.index.classnames=

#  Default set of decorators
event.query.data.decorators=

#  Configure max results for Event Query only, -1 means unlimited
event.query.max.results=-1

############################
#
# Cached Results
#
############################
cached.results.hdfs.uri=hdfs://localhost:8020/
cached.results.export.dir=/CachedResults
#  Number of rows per batch update in CachedResults.load
cached_results.rows.per.batch=10
#  Number of days that the cached results tables should remain in the cached results store
cached_results.daysToLive=1

############################
#
# LookupUUID
#
############################
#  Default uuid lookup mappings
lookup.uuid.mappings=
#  Default uuidTypes
lookup.uuid.uuidTypes=
#  Default lookup.uuid.beginDate
lookup.uuid.beginDate=20100101

num.shards=241

############################
#
# Atom Service
#
############################
ATOM_TABLENAME=Atom

############################
#
# MapReduce Service
#
############################
#  Default restrict input formats
mapReduce.inputFormat.restrict=true
mapReduce.job.tracker=localhost:8021
#mapreduce.http.port identifies Wildfly address and port
mapReduce.http.port=http://localhost:8443
mapReduce.hdfs.uri=hdfs://localhost:8020/
mapReduce.hdfs.base.dir=/MapReduceService

bulkResults.job.tracker=localhost:8021
bulkResults.http.port=http://localhost:8080
bulkResults.hdfs.uri=hdfs://localhost:8020/
bulkResults.hdfs.base.dir=/BulkResults

#  Query configuration parameter, true by default, but may cause an issue with malformed UIDs
include.hierarchy.fields=false
hierarchy.field.options=

#  BaseEventQuery (beq) thresholds
beq.baseIteratorPriority=100
beq.eventPerDayThreshold=40000
beq.shardsPerDayThreshold=20
# max number of terms AFTER all expansions (calculated based on how much the initial parser can handle before hitting a stack overflow: between 3500 and 3750)
beq.maxTermThreshold=2000
# max depth of query (calculated based on how much the initial parser can handle before hitting a stack overflow: between 3500 and 3750)
beq.maxDepthThreshold=2000
# only used in the refactored query logic: max value (regex/range) expansion and max unfielded (_ANYFIELD) expansion
beq.valueExpansionThreshold=50
beq.unfieldedExpansionThreshold=50
# only used in the refactored query logic: max or'ed values for a single field after which an iverator is used.  FSTs are used after if greater than both.
beq.orExpansionThreshold=500
beq.orExpansionFstThreshold=750
# only used in the legacy query logic: max ranges and max terms post expansion
beq.rangeExpansionThreshold=2000
beq.maxTermExpansionThreshold=2000
# The max number of splits to divide a range into for the ivarators.  They are run in a pool of threads controlled by the tserver.datawave.ivarator.threads accumulo configuration property which defaults to 100 (IteratorThreadPoolManager).
beq.fieldIndexRangeSplit=16
# The max number of sources that can be created across ivarators for one scan
beq.maxIvaratorSources=20
# The max number of files that one ivarator can open at one time
beq.maxIvaratorOpenFiles=100
# The max number of evaluation pipelines.  They are run in a pool of threads controlled by the tserver.datawave.evaluation.threads accumulo configuration property which defaults to 100 (IteratorThreadPoolManager).
beq.evaluationPipelines=16
# The max number of non-null evaluated results to cache on each tserver beyond the evaluation pipelines in queue
beq.pipelineCachedResults=16
# Are full scans enabled for the base event query?
beq.fullTableScanEnabled=false

#  Threads used for various query logics
shard.query.threads=100
index.query.threads=100
date.index.threads=20
edge.query.threads=16

# MySQL Connection settings parameters
mysql.host=localhost
mysql.dbname=sort
mysql.pool.min.size=5
mysql.pool.max.size=20


extra.connection.factory.entries=

#Table for storing Atom entries created by the AtomDataTypeHandler
atom.table.name=Atom
atom.connection.pool.name=WAREHOUSE

# HDFS backed sorted set Settings
hdfs.site.config.urls=file:///etc/hadoop/conf/core-site.xml,file:///etc/hadoop/conf/hdfs-site.xml
ivarator.cache.base.uris=hdfs:///IvaratorCache
ivarator.fst.hdfs.base.uris=hdfs:///IvaratorCache
ivarator.zookeeper.hosts=

id.translation.return.fields=

jboss.log.dir=
jboss.data.dir=

############################
#
# Query Metrics Settings
#
############################
query.metrics.table.basename=QueryMetrics
querymetrics.policy.enforcer.class=datawave.policy.IngestPolicyEnforcer$NoOpIngestPolicyEnforcer
query.metrics.marking=
query.metrics.visibility=

############################
#
# Mutable Metadata Settings
#
############################
mutable.metadata.index.only.mapping=
mutable.metadata.index.only.suffixes=
mutable.metadata.content.fields=


metrics.warehouse.namenode=
metrics.warehouse.hadoop.path=

cache.accumulo.username=
cache.accumulo.password=
cache.accumulo.zookeepers=
cache.accumulo.instance=

EDGE_EVALUATE_PRECONDITIONS=false
EDGE_DEFINITION_FILE=config/EdgeSpringConfig.xml

COMPOSITE_INGEST_DATA_TYPES=
DEPRECATED_INGEST_DATA_TYPES=
DATE_INDEX_TABLE_NAME=dateIndex
NUM_DATE_INDEX_SHARDS=10
PASSWORD_INGEST_ENV=/opt/datawave-ingest/ingest-passwd.sh

INCLUDE_UID_TIME_COMPONENT=false
LOADDATES_TABLE_NAME=LoadDates
LOADDATES_TABLE_ENABLED=true
LIVE_FLAG_TIMEOUT_MS=10000
BULK_FLAG_TIMEOUT_MS=480000

# These should be set only if deploying on the CDH distro of Accumulo,
# otherwise leave them blank
WAREHOUSE_ACCUMULO_LIB=
WAREHOUSE_ACCUMULO_BIN=

mutableMetadata.securityMarkingExemptFields=

####################################
# Internal Edge Model Defaults
#
# Allows the edge query model to be dictated by the needs of the deployment environment. 
# Addtionally, the default field names defined here can be overridden by the use of custom
# query models as with the event-based query logics 
#
####################################
edge.model.base.map= \
\n    <util:map id="baseFieldMap" key-type="java.lang.String" value-type="java.lang.String"> \
\n           <entry key="EDGE_SOURCE" value="SOURCE" /> \
\n           <entry key="EDGE_SINK" value="SINK"/> \
\n           <entry key="EDGE_TYPE" value="TYPE"/> \
\n           <entry key="EDGE_RELATIONSHIP" value="RELATION"/> \
\n           <entry key="EDGE_ATTRIBUTE1" value="ATTRIBUTE1"/> \
\n           <entry key="EDGE_ATTRIBUTE2" value="ATTRIBUTE2"/> \
\n           <entry key="EDGE_ATTRIBUTE3" value="ATTRIBUTE3"/> \
\n           <entry key="DATE" value="DATE"/> \
\n           <entry key="STATS_EDGE" value="STATS_TYPE"/> \
\n    </util:map>
edge.model.keyutil.map= \
\n    <util:map id="keyUtilFieldMap" key-type="java.lang.String" value-type="java.lang.String"> \
\n           <entry key="ENRICHMENT_TYPE" value="ENRICHMENT_TYPE"/> \
\n           <entry key="FACT_TYPE" value="FACT_TYPE"/> \
\n           <entry key="GROUPED_FIELDS" value="GROUPED_FIELDS"/> \
\n    </util:map>
edge.model.transform.map= \
\n    <util:map id="transformFieldMap" key-type="java.lang.String" value-type="java.lang.String"> \
\n           <entry key="COUNT" value="COUNT"/> \
\n           <entry key="COUNTS" value="COUNTS"/> \
\n           <entry key="LOAD_DATE" value="LOADDATE"/> \
\n           <entry key="ACTIVITY_DATE" value="ACTIVITY_DATE"/> \
\n    </util:map>

metadata.table.names= \
  <value>DatawaveMetadata</value> \
  \n  <value>knowledgeMetadata</value> \
  \n  <value>errorMetadata</value>

# Comma-separated list of auths needed for internal queries against DW's metadata table 
metadatahelper.default.auths=

# Comma-separated list of valid OU values that denote an "NPE" (server) DN
security.npe.ou.entries=

############################
#
# TypeMetadata
#
############################
type.metadata.hdfs.uri=hdfs://localhost:8020/
type.metadata.dir=/TypeMetadata
type.metadata.fileName=typeMetadata


##########################
#
# UID Caching
#
##########################
SNOWFLAKE_ZOOKEEPER_ENABLED=false
SNOWFLAKE_ZOOKEEPERS=

##########################
#
# Timely Defaults
#
##########################

timely.host=localhost
timely.tcp.port=4242
timely.udp.port=4245
query.metrics.timelyMetricTags= \
\n     <util:set value-type="java.lang.String"> \
\n         <value>USER</value> \
\n         <value>HOST</value> \
\n         <value>QUERY_ID</value> \
\n         <value>QUERY_LOGIC</value> \
\n     </util:set>
