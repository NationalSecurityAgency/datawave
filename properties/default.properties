#  Passwords should be set in private maven settings
#  They are here for reference but commented out so that the
#  assert-properties plugin will warn the user if they are not set

############################
#
# Usernames
#
############################
accumulo.user.name=root
mysql.user.name=sorted
#  Admin Console and JMX Console username
jboss.jmx.username=jmxadmin
#  Credentials for HORNETQ servers to join the cluster
hornetq.cluster.username=datawave-cluster
#  Your system's default username for JMS. This will be used by MDBs and other clients to connect to JMS Server. You can also create
#  additional accounts for external systems by adding entries in hornetq-users.properties and hornetq-roles.properties
hornetq.system.username=DATAWAVE

############################
#
# Passwords
#
############################
#server.keystore.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#accumulo.user.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#mysql.user.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#jboss.jmx.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#hornetq.cluster.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
#hornetq.system.password=SET_ME_IN_PRIVATE_MAVEN_SETTINGS
server.truststore.password=

############################
#
# Server Identity
#
############################
server.cert.basename=
# Subject DN of the server cert
server.dn=
# Issuer DN of the server cert
issuer.dn=
server.trust.store=

############################
#
# Security Settings
#
############################
# Whether or not to use the remote authorization service
security.use.remoteauthservice=false
# Whether or not to use the test authorization service that loads canned users
security.use.testauthservice=false
# Spring context entry defining the location of test authorization service entries
security.testauthservice.context.entry=
# JSON-encoded DatawaveUser objects to use in the test authorization service
security.testauthservice.users=
# Configuration for the remote DatawaveUser service
#
# Find the host and port of the service using a SRV DNS lookup
security.remoteuserservice.srv.lookup.enabled=false
# The DNS servers to use for the SRV lookup
security.remoteuserservice.srv.lookup.servers=127.0.0.1
# The port on which the DNS server that serves SRV records is listening
security.remoteuserservice.srv.lookup.port=8600
# The scheme to use when connecting to the remote user service
security.remoteuserservice.scheme=https
# The host to connect to (or do a SRV lookup on) for the remote user service
security.remoteuserservice.host=localhost
# The port to connect to (unless a SRV lookup was performed) for the remote user service
security.remoteuserservice.port=8643

############################
#
# Configuration for the remote Accumulo service
#
############################
# Find the host and port of the service using a SRV DNS lookup
accumulo.remoteservice.srv.lookup.enabled=false
# The DNS servers to use for the SRV lookup
accumulo.remoteservice.srv.lookup.servers=127.0.0.1
# The port on which the DNS server that serves SRV records is listening
accumulo.remoteservice.srv.lookup.port=8600
# The scheme to use when connecting to the remote user service
accumulo.remoteservice.scheme=https
# The host to connect to (or do a SRV lookup on) for the remote user service
accumulo.remoteservice.host=localhost
# The port to connect to (unless a SRV lookup was performed) for the remote user service
accumulo.remoteservice.port=8943

############################
#
# Configuration for the remote Query Metric service
#
############################
# Find the host and port of the service using a SRV DNS lookup
querymetric.remoteservice.srv.lookup.enabled=false
# The DNS servers to use for the SRV lookup
querymetric.remoteservice.srv.lookup.servers=127.0.0.1
# The port on which the DNS server that serves SRV records is listening
querymetric.remoteservice.srv.lookup.port=8600
# The scheme to use when connecting to the remote query metric service
querymetric.remoteservice.scheme=https
# The host to connect to (or do a SRV lookup on) for the remote query metric service
querymetric.remoteservice.host=localhost
# The port to connect to (unless a SRV lookup was performed) for the remote query metric service
querymetric.remoteservice.port=9043

############################
#
# Server Settings
#
############################
jboss.console.redirect=
port.definition.set=ports-default
#  JBoss Heap size, used in bin/run.conf
jboss.jvm.heap.size=4096m
#  JBoss CMSInitiatingOccupancyFraction, start a garbage collection if the tenured generation exceeds this fraction, 92% by default
jboss.cms.initiating.occupancy.fraction=75
#  Additional args for the JBoss JVM used in bin/run.conf
jboss.java.opts=
# Add no additional JAVA_OPTS to run.conf
jboss.extra.java.opts=
#  Extra stuff to append to the end of run.conf
jboss.run.conf.extras=
#location of JBoss log dir
jboss.log.hdfs.uri=hdfs://localhost:8020/
jboss.log.hdfs.dir=/datawave/WebServiceLogs
# Application Server Cluster partition name
jboss.partition.name=${env.USER}
#user the wildfly init.d script should use to run wildfly
jboss.runas.user=jboss

# Defines the size parameters of the worker's task thread pool
# Suggest setting values here based on accumulo connection pool sizes, available cores, and expected access patterns
wildfly.io.worker.default.task-max-threads=16
# How many I/O (selector) threads should be maintained. Generally this number should be a small constant multiple of the number of available cores.
wildfly.io.worker.default.io-threads=2

############################
#
# RestEasy  Settings
#
############################
#  Number of job result sets held in memory at once, defaults to 100
resteasy.async.job.service.max.job.results=200
#  Maximum wait time on a job when a client is querying for it, defaults to 5m in ms
resteasy.async.job.service.max.wait=300000
#  Thread pool size of background threads that run the job, defaults to 100
resteasy.async.job.service.thread.pool.size=200
#  The base path for job URIs
resteasy.async.job.service.base.path=/asynch/jobs

############################
#
# Table & Query Settings
#
############################

table.name.metadata=datawave.metadata
table.name.shard=datawave.shard
table.name.shardStats=datawave.shardStats
table.name.shardIndex=datawave.shardIndex
table.name.shardReverseIndex=datawave.shardReverseIndex
table.name.dateIndex=datawave.dateIndex
table.name.edge=datawave.edge
table.name.errors.metadata=datawave.error_m
table.name.errors.shardIndex=datawave.error_i
table.name.errors.shardReverseIndex=datawave.error_r
table.name.errors.shard=datawave.error_s
table.name.queryMetrics.metadata=datawave.queryMetrics_m
table.name.queryMetrics.shardIndex=datawave.queryMetrics_i
table.name.queryMetrics.shardReverseIndex=datawave.queryMetrics_r
table.name.queryMetrics.shard=datawave.queryMetrics_s
table.name.queryMetrics.dateIndex=datawave.queryMetrics_di
table.name.loadDates=datawave.loadDates
table.name.atom.categories=datawave.atom
table.name.facet=datawave.facets
table.name.facet.metadata=datawave.facetMetadata
table.name.facet.hashes=datawave.facetHashes

table.shard.numShardsPerDay=10
table.dateIndex.numShardsPerDay=10
table.loadDates.enabled=true

metadata.table.names= \
<value>datawave.metadata</value> \
\n  <value>datawave.queryMetrics_m</value> \
\n  <value>datawave.error_m</value>

tables.to.cache=datawave.metadata,datawave.queryMetrics_m,datawave.error_m
cache.reloadInterval=86400000

indexTables.keepCountOnlyEntries=false

default.date.type.name=EVENT

# Number of minutes that a query can be idle before the connection is closed
query.expiration.minutes=15
# Number of minutes that a query next or create call can take before it is canceled.
query.max.call.time.minutes=60
# Number of minutes after which the page will be returned iff it contains results.  This prevents a query from being cancelled re query.max.call.time.minutes if there are results.
query.page.shortcircuit.minutes=55
# Number of minutes after which the page size velocity will be checked (percent page full vs percent call time complete) to potentially short circuit the next call
query.page.size.shortcircuit.minutes=30
# The max page size that a user can request.
query.default.page.size=10
# The max page size that a user can request.  0 turns off this feature
query.max.page.size=10000
# The number of bytes at which a page will be returned, event if the pagesize has not been reached.  0 turns off this feature
query.page.byte.trigger=0
# Determine whether or not we collapse UIDS into a sharded range when doing the rangestream lookup
query.collapse.uids=false
# If we have more UIDS than this threshold, collapse into a single rangestream lookup.
query.collapse.uids.threshold=-1
# Determine when we give up on an global index scan and push down to the field index.  Default is virtually unlimited (1 year).
query.max.index.scan.ms=31536000000
# Suppresses documents which would otherwise have only index only fields within it
disable.index.only.documents=false
# Indicates whether index-only filter functions should be enabled, such as filter:includeRegex()
enable.index.only.filter.functions=false
query.tld.collapse.uids=false
#fields generated internally at query evaluation time
evaluation.only.fields=
############################
#
# Accumulo Connection Pools
#
############################
accumulo.instance.name=accumulo
zookeeper.hosts=localhost:2181

# Number of connections in the connection pools to the accumulo instance. If not enough connections, then operations will block
# until a connection becomes available. Be careful here as when used in a batch scanner, a connection will use N threads and network
# connections when querying ACCUMULO.These are the defaults, if your table names are different then override them in
# your profile
accumulo.low.defaultpool.size=25
accumulo.normal.defaultpool.size=50
accumulo.high.defaultpool.size=100
accumulo.admin.defaultpool.size=200
accumulo.low.uuidpool.size=1
accumulo.normal.uuidpool.size=2
accumulo.high.uuidpool.size=3
accumulo.admin.uuidpool.size=5
accumulo.low.fipool.size=1
accumulo.normal.fipool.size=2
accumulo.high.fipool.size=3
accumulo.admin.fipool.size=5

############################
#
# EJB Settings
#
############################
# Number of MDBs in the pool for the modification cache, max jboss.mdb.pool.max.size.  This will determine the number of concurrent calls to the mutable field cache
modification.cache.mdb.pool.size=50
# Number of threads available for EJB3 asynchronous methods
jboss.ejb3.async.threads=10
# Number of seconds before transactions will time out (NOTE: This should ne &gt; query.max.call.time.minutes)
jboss.transaction.time.out=3900
# Number of ms before the remote ejb connections will time out (NOTE: This should be &gt; query.max.call.time.minutes)
jboss.ejb3.connector.time.out=3900000
# Number of threads for accepting HTTP requests, defaults to 200
jboss.web.max.threads=200
# Number of requests to queue up for available thread. When queue is full then connection refused errors will be returned to the caller
jboss.web.accept.count=200
# Maximum number of Stateless Session Bean instances in each pool
jboss.slsb.pool.max.size=200
# Timeout (ms) before throwing an exception when waiting to get a Stateless Session Bean instance from the pool
jboss.slsb.pool.timeout=30000
# Maximum number of Message Driven Bean instances in each pool
jboss.mdb.pool.max.size=200
# Timeout (ms) before throwing an exception when waiting to get a Message Driven Bean instance from the pool
jboss.mdb.pool.timeout=30000
# Number of threads to be used by the managed executor service (increase this if seeing RejectedExecutionExceptions)
jboss.managed.executor.service.default.max.threads=32

############################
#
# HornetQ Settings
#
############################
# HORNETQ JMS DataSource max pool size
hornetq.datasource.max.pool.size=200
hornetq.host=
hornetq.port=

############################
#
# DATAWAVE Settings
#
############################
# Transport guarantee for web apps
webapp.transport.guarantee=CONFIDENTIAL
# Tell the login module to expect client cert, and not DN stuffed in a header.
trusted.header.login=false
# web service response namespaces
datawave.webservice.namespace=http://webservice.datawave/v1
# Name of the Cluster
cluster.name=DEV

############################
#
# Timely metrics reporting
#
############################
metrics.reporter.host=localhost
metrics.reporter.port=54321
metrics.reporter.class=datawave.metrics.TimelyMetricsReporterFactory

############################
#
# EventQuery
#
############################
# Default set of filter properties (which are disabled)
event.query.filters.enabled=false
event.query.filters.classnames=
event.query.filters.options=
event.query.filters.index.classnames=

# Default set of decorators
event.query.data.decorators=

# Configure max results for Event Query only, -1 means unlimited
event.query.max.results=-1

############################
#
# Cached Results
#
############################
cached.results.hdfs.uri=hdfs://localhost:8020/
cached.results.export.dir=/CachedResults
# Number of rows per batch update in CachedResults.load
cached_results.rows.per.batch=10
# Number of days that the cached results tables should remain in the cached results store
cached_results.daysToLive=1

############################
#
# LookupUUID
#
############################
# Default uuid lookup mappings
lookup.uuid.mappings=
# Default uuidTypes
lookup.uuid.uuidTypes=
# Default lookup.uuid.beginDate
lookup.uuid.beginDate=20100101

############################
#
# MapReduce Service
#
############################
# Default restrict input formats
mapReduce.inputFormat.restrict=true
mapReduce.job.tracker=localhost:8021
#mapreduce.http.port identifies Wildfly address and port
mapReduce.http.port=http://localhost:8443
mapReduce.hdfs.uri=hdfs://localhost:8020/
mapReduce.hdfs.base.dir=/datawave/MapReduceService

bulkResults.job.tracker=localhost:8021
bulkResults.http.port=http://localhost:8080
bulkResults.hdfs.uri=hdfs://localhost:8020/
bulkResults.hdfs.base.dir=/datawave/BulkResults
mapreduce.securitydomain.useJobCache=true

# Query configuration parameter, true by default, but may cause an issue with malformed UIDs
include.hierarchy.fields=false
hierarchy.field.options=

# BaseEventQuery (beq) thresholds
beq.baseIteratorPriority=100
beq.eventPerDayThreshold=40000
beq.shardsPerDayThreshold=20
# max number of terms AFTER all expansions (calculated based on how much the initial parser can handle before hitting a stack overflow: between 3500 and 3750)
beq.maxTermThreshold=2000
# max depth of query (calculated based on how much the initial parser can handle before hitting a stack overflow: between 3500 and 3750)
beq.maxDepthThreshold=2000
# only used in the refactored query logic: max value (regex/range) expansion and max unfielded (_ANYFIELD) expansion
beq.valueExpansionThreshold=50
beq.unfieldedExpansionThreshold=50
# only used in the refactored query logic: max or'ed values for a single field after which an iverator is used.  FSTs are used after if greater than both.
beq.orExpansionThreshold=500
# the maximum number of allowed ranges against a single field within an or node before combining ranges into the desired number of ivarators.
beq.orRangeThreshold=10
# The maximum number of ranges to combine for merged range ivarators against a single field within an or node
beq.maxRangesPerRangeIvarator=5
# The maximum number of range ivarators allowed for a single field under an or node
beq.maxOrRangeIvarators=10
beq.orExpansionFstThreshold=750
# only used in the legacy query logic: max ranges and max terms post expansion
beq.rangeExpansionThreshold=2000
beq.maxTermExpansionThreshold=2000
# The max number of splits to divide a range into for the ivarators.  They are run in a pool of threads controlled by the tserver.datawave.ivarator.threads accumulo configuration property which defaults to 100 (IteratorThreadPoolManager).
beq.fieldIndexRangeSplit=16
# The max number of sources that can be created across ivarators for one scan
beq.maxIvaratorSources=20
# The max number of files that one ivarator can open at one time
beq.maxIvaratorOpenFiles=100
# The max number of evaluation pipelines.  They are run in a pool of threads controlled by the tserver.datawave.evaluation.threads accumulo configuration property which defaults to 100 (IteratorThreadPoolManager).
beq.evaluationPipelines=16
# The max number of non-null evaluated results to cache on each tserver beyond the evaluation pipelines in queue
beq.pipelineCachedResults=16
# Are full scans enabled for the base event query?
beq.fullTableScanEnabled=false

# Threads used for various query logics
shard.query.threads=100
index.query.threads=100
date.index.threads=20
edge.query.threads=16

# MySQL Connection settings parameters
mysql.host=localhost
mysql.dbname=sort
mysql.pool.min.size=5
mysql.pool.max.size=20

extra.connection.factory.entries=

# Web service connection pool for atom service
atom.connection.pool.name=WAREHOUSE

# HDFS backed sorted set Settings
hdfs.site.config.urls=file:///etc/hadoop/conf/core-site.xml,file:///etc/hadoop/conf/hdfs-site.xml

# The paths to use for the ivarators.  Paths will be used in the order they are listed.
## Specify a list of beans using the 1, 2 or 3-argument constructor with the following params:
## 1) basePathURI - (Required) A string URI representing the filesystem and directory to use for the ivarators.
##        The path should be fully qualified and start with either 'file:/' or 'hdfs:/'.
## 2) priority - (Optional, Default: Integer.MAX_VALUE) An integer >= 0 which can be used to give certain ivarator
##        paths preference over others.  For paths which share the same priority, the order will be determined at
##        random.  Otherwise, paths are sorted in ascending order by priority.
## 3) The third argument can be specified either as an exact number of MB (long), or as a percent (double).
##   - minAvailableStorageMB - (Optional, Default: 0) A long, greater than or equal to 0, which specifies the minimum
##        amount of available storage space required to persist to this ivarator path.  If less than this amount is
##        available, we will not use this ivarator path.
##   - minAvailableStoragePercent - (Optional, Default: 0.0) A double, between 0.0 and 1.0, which specifies the
##        minimum percent of available storage space required to persist to this ivarator path.  If less than this
##        percent is available, we will not use this ivarator path.
ivarator.cache.dir.config= \
          <bean class="datawave.query.iterator.ivarator.IvaratorCacheDirConfig"> \
\n            <constructor-arg value="hdfs:///IvaratorCache" /> \
\n        </bean>

# By comparison, here is an example configuration which specifies two ivarator cache dirs.
# - The first cache directory is on the local disk, has a priority of 0, and has minAvailableStorageMB set to 4096.
# - The second cache directory is in hdfs, has a priority of 1, and has minAvailableStoragePercent set to 0.33 (i.e. 33%)
#ivarator.cache.dir.config= \
#          <bean class="datawave.query.iterator.ivarator.IvaratorCacheDirConfig"> \
#\n            <constructor-arg value="file:///IvaratorCache" /> \
#\n            <constructor-arg value="0" type="int" /> \
#\n            <constructor-arg value="4096" type="long" /> \
#\n        </bean> \
#\n        <bean class="datawave.query.iterator.ivarator.IvaratorCacheDirConfig"> \
#\n            <constructor-arg value="hdfs:///IvaratorCache" /> \
#\n            <constructor-arg value="1" type="int" /> \
#\n            <constructor-arg value="0.33" type="double" /> \
#\n        </bean>

ivarator.fst.hdfs.base.uris=hdfs:///IvaratorCache
ivarator.zookeeper.hosts=

id.translation.return.fields=

jboss.log.dir=
jboss.data.dir=

############################
#
# Query Metrics Settings
#
############################
query.metrics.ingest.policy.enforcer.class=datawave.policy.IngestPolicyEnforcer$NoOpIngestPolicyEnforcer
query.metrics.marking=
query.metrics.visibility=

############################
#
# Mutable Metadata Settings
#
############################
mutable.metadata.index.only.mapping=
mutable.metadata.index.only.suffixes=
mutable.metadata.content.fields=

metrics.warehouse.namenode=
metrics.warehouse.hadoop.path=

cache.accumulo.username=
cache.accumulo.password=
cache.accumulo.zookeepers=
cache.accumulo.instance=

DATAWAVE_INGEST_HOME=/opt/datawave-ingest

EDGE_EVALUATE_PRECONDITIONS=false
EDGE_DEFINITION_FILE=config/edge-definitions.xml

COMPOSITE_INGEST_DATA_TYPES=
DEPRECATED_INGEST_DATA_TYPES=
PASSWORD_INGEST_ENV=/opt/datawave-ingest/ingest-passwd.sh

INCLUDE_UID_TIME_COMPONENT=false

LIVE_FLAG_TIMEOUT_MS=10000
BULK_FLAG_TIMEOUT_MS=480000

LIVE_FLAG_COLLECT_METRICS=false
BULK_FLAG_COLLECT_METRICS=false

FLAG_EXTRA_ARGS=
MAP_FILE_LOADER_EXTRA_ARGS=-ingestMetricsDisabled

JOB_OBSERVERS=
JOB_OBSERVER_EXTRA_OPTS=

# These should be set only if deploying on the CDH distro of Accumulo,
# otherwise leave them blank
WAREHOUSE_ACCUMULO_LIB=
WAREHOUSE_ACCUMULO_BIN=

mutableMetadata.securityMarkingExemptFields=

####################################
# Internal Edge Model Defaults
#
# Allows the edge query model to be dictated by the needs of the deployment environment.
# Addtionally, the default field names defined here can be overridden by the use of custom
# query models as with the event-based query logics
#
####################################
edge.model.base.map= \
\n    <util:map id="baseFieldMap" key-type="java.lang.String" value-type="java.lang.String"> \
\n           <entry key="EDGE_SOURCE" value="SOURCE" /> \
\n           <entry key="EDGE_SINK" value="SINK"/> \
\n           <entry key="EDGE_TYPE" value="TYPE"/> \
\n           <entry key="EDGE_RELATIONSHIP" value="RELATION"/> \
\n           <entry key="EDGE_ATTRIBUTE1" value="ATTRIBUTE1"/> \
\n           <entry key="EDGE_ATTRIBUTE2" value="ATTRIBUTE2"/> \
\n           <entry key="EDGE_ATTRIBUTE3" value="ATTRIBUTE3"/> \
\n           <entry key="DATE" value="DATE"/> \
\n           <entry key="STATS_EDGE" value="STATS_TYPE"/> \
\n    </util:map>
edge.model.keyutil.map= \
\n    <util:map id="keyUtilFieldMap" key-type="java.lang.String" value-type="java.lang.String"> \
\n           <entry key="ENRICHMENT_TYPE" value="ENRICHMENT_TYPE"/> \
\n           <entry key="FACT_TYPE" value="FACT_TYPE"/> \
\n           <entry key="GROUPED_FIELDS" value="GROUPED_FIELDS"/> \
\n    </util:map>
edge.model.transform.map= \
\n    <util:map id="transformFieldMap" key-type="java.lang.String" value-type="java.lang.String"> \
\n           <entry key="COUNT" value="COUNT"/> \
\n           <entry key="COUNTS" value="COUNTS"/> \
\n           <entry key="LOAD_DATE" value="LOADDATE"/> \
\n           <entry key="ACTIVITY_DATE" value="ACTIVITY_DATE"/> \
\n    </util:map>

# Comma-separated list of auths needed for internal queries against DW's metadata table
metadatahelper.default.auths=

# Comma-separated list of valid OU values that denote an "NPE" (server) DN.
# This should be overridden as needed in the deployment environment to reflect
# PKI validation requirements there.
security.npe.ou.entries=OVERRIDE_ME_IN_ENVIRONMENT_PROFILE_PROPERTIES

# Regex pattern denoting a valid subject DN. This should be overridden as needed in the
# deployment environment to reflect PKI validation requirements there.
security.subject.dn.pattern=OVERRIDE_ME_IN_ENVIRONMENT_PROFILE_PROPERTIES

############################
#
# TypeMetadata
#
############################
type.metadata.hdfs.uri=hdfs://localhost:8020/
type.metadata.dir=/datawave/TypeMetadata
type.metadata.fileName=typeMetadata

##########################
#
# UID Caching
#
##########################
SNOWFLAKE_ZOOKEEPER_ENABLED=false
SNOWFLAKE_ZOOKEEPERS=

##########################
#
# Timely Defaults
#
##########################

timely.host=localhost
timely.tcp.port=4242
timely.udp.port=4245
query.metrics.timelyMetricTags= \
\n     <util:set value-type="java.lang.String"> \
\n         <value>USER</value> \
\n         <value>HOST</value> \
\n         <value>QUERY_ID</value> \
\n         <value>QUERY_LOGIC</value> \
\n     </util:set>

##########################
#
# Extra DataWave Docs Menu Items
#
##########################

datawave.docs.menu.extras=

##########################
#
# Basemap Configuration for Query Geometry Map
#
# Add key-value pairings of layer name to leaflet tile layers.  The first
# key-value pairing will be used as the default basemap.
#
# In order to use the example provided below, you will need to supply your
# own mapbox access token.
#
##########################
basemaps= {\
          'Mapbox Streets': L.tileLayer( \
              'https://api.tiles.mapbox.com/v4/{id}/{z}/{x}/{y}.png?access_token={accessToken}', \
              { \
                  maxZoom: 18, \
                  id: 'mapbox.streets', \
                  accessToken: 'your.mapbox.access.token' \
              }), \
          'Mapbox Satellite': L.tileLayer( \
              'https://api.tiles.mapbox.com/v4/{id}/{z}/{x}/{y}.png?access_token={accessToken}', \
              { \
                  maxZoom: 18, \
                  id: 'mapbox.satellite', \
                  accessToken: 'your.mapbox.access.token' \
              }) \
          }
