CONFIGURATION=test
RCPT_TO=hadoop@localhost

docker.image.prefix=

# ingest properties
DATAWAVE_INGEST_HOME=/local/datawave-ingest

WAREHOUSE_ACCUMULO_HOME=/local/accumulo-current
WAREHOUSE_HDFS_NAME_NODE=hdfs://localhost:9000
WAREHOUSE_JOBTRACKER_NODE=localhost:8032
WAREHOUSE_ZOOKEEPERS=localhost:2181
WAREHOUSE_INSTANCE_NAME=SuperStar
#Sets variable sets the zookeeper location for the warehouse side
zookeeper.hosts=localhost:2181

INGEST_ACCUMULO_HOME=/local/accumulo-current
INGEST_HDFS_NAME_NODE=hdfs://localhost:9000
INGEST_JOBTRACKER_NODE=localhost:8032
INGEST_ZOOKEEPERS=localhost:2181
INGEST_INSTANCE_NAME=SuperStar

JOB_CACHE_REPLICATION=1

STAGING_HOSTS=localhost
DUMPER_HOSTS=localhost
INGEST_HOST=localhost
ROLLUP_HOST=localhost

#extra mapreduce options (e.g. mapreduce.task.io.sort.mb and the like)
MAPRED_INGEST_OPTS=-useInlineCombiner

#extra HADOOP_OPTS (java options)
HADOOP_INGEST_OPTS=

#extra CHILD_OPTS (java options)
CHILD_INGEST_OPTS=

BULK_CHILD_MAP_MAX_MEMORY_MB=2048
LIVE_CHILD_MAP_MAX_MEMORY_MB=1024
BULK_CHILD_REDUCE_MAX_MEMORY_MB=2048
LIVE_CHILD_REDUCE_MAX_MEMORY_MB=1024

BULK_INGEST_DATA_TYPES=shardStats,wikipedia,mycsv,myjson
LIVE_INGEST_DATA_TYPES=wikipedia,mycsv,myjson

# Clear out these values if you do not want standard shard ingest.
DEFAULT_SHARD_HANDLER_CLASSES=datawave.ingest.mapreduce.handler.shard.AbstractColumnBasedHandler
ALL_HANDLER_CLASSES=datawave.ingest.mapreduce.handler.edge.ProtobufEdgeDataTypeHandler,datawave.ingest.mapreduce.handler.dateindex.DateIndexDataTypeHandler

BULK_INGEST_REDUCERS=10
LIVE_INGEST_REDUCERS=10

# Note the max blocks per job must be less than or equal to the number of mappers
INGEST_BULK_JOBS=1
INGEST_BULK_MAPPERS=4
INGEST_MAX_BULK_BLOCKS_PER_JOB=4
INGEST_LIVE_JOBS=1
INGEST_LIVE_MAPPERS=4
INGEST_MAX_LIVE_BLOCKS_PER_JOB=4

INDEX_STATS_MAX_MAPPERS=7

NUM_MAP_LOADERS=1

USERNAME=root
PASSWORD=secret

ZOOKEEPER_HOME=/local/zookeeper
HADOOP_HOME=/local/hadoop
MAPRED_HOME=/local/hadoop

WAREHOUSE_HADOOP_CONF=/local/hadoop/conf
INGEST_HADOOP_CONF=/local/hadoop/conf

HDFS_BASE_DIR=/NewIngest

MONITOR_SERVER_HOST=localhost

LOG_DIR=/local/logs/ingest
PDSH_LOG_DIR=${LOG_DIR}/pdsh_logs
FLAG_DIR=/local/data/flags
FLAG_MAKER_CONFIG=/local/datawave-ingest/config/liveFlagMaker.xml,/local/datawave-ingest/config/bulkFlagMaker.xml,/local/datawave-ingest/config/WikipediaFlagMakerConfig.xml
BIN_DIR_FOR_FLAGS=/local/datawave-ingest/bin

PYTHON=/usr/bin/python

# Setting discard interval to 0 in order to disable auto-ageoff @ ingest time
EVENT_DISCARD_INTERVAL=0

# Setting discard interval to 0 in order to disable auto-ageoff @ ingest time
EVENT_DISCARD_FUTURE_INTERVAL=0

DATAWAVE_CACHE_PORT=20444

EDGE_DEFINITION_FILE=config/edge-definitions.xml

ERROR_TABLE=errors
ANALYTIC_MTX=analytic_metrics
LOADER_MTX=loader_metrics
INGEST_MTX=ingest_metrics
BULK_INGEST_METRIC_THRESHOLD=1500000
LIVE_INGEST_METRIC_THRESHOLD=1500000

KEYSTORE=/local/wildfly/standalone/configuration/certificates/testServer.p12
KEYSTORE_TYPE=PKCS12
KEYSTORE_PASSWORD=secret
TRUSTSTORE=/local/wildfly/standalone/configuration/certificates/ca.jks

FLAG_METRICS_DIR=/data/flagMetrics
TRUSTSTORE_PASSWORD=Changeit1
TRUSTSTORE_TYPE=JKS

cluster.name=dev
accumulo.instance.name=SuperStar
accumulo.user.name=root
accumulo.user.password=secret
# uncomment to pull password from the environment. also need to modify bootstrap-web.sh for quickstart
#accumulo.user.password=env:DW_ACCUMULO_PASSWORD

cached.results.hdfs.uri=hdfs://localhost:8020/
cached.results.export.dir=/CachedResults

lock.file.dir=/tmp/datawave-lock-files/
JAVA_HOME=/local/jdk1.8.0

# query properties
server.keystore.password=secret
mysql.user.password=datawave
jboss.jmx.password=blah
hornetq.cluster.password=blah
hornetq.system.password=blah

server.truststore.password=Changeit

#Sets up the Atom Service
atom.wildfly.hostname=localhost
atom.wildfly.port.number=8443
atom.connection.pool.name=WAREHOUSE


security.use.testauthservice=true
security.testauthservice.context.entry=<value>classpath*:datawave/security/TestDatawaveUserServiceConfiguration.xml</value>
security.testauthservice.users= \
\n        <value><![CDATA[ \
\n        { \
\n            "dn": { \
\n                "subjectDN": "cn=test a. user, ou=my department, o=my company, st=some-state, c=us", \
\n                "issuerDN": "cn=test ca, ou=my department, o=my company, st=some-state, c=us" \
\n            }, \
\n            "userType": "USER",\
\n            "auths": [ "PVT", "PUB" ], \
\n            "roles": [ "PRIVATE", "PUBLIC", "Administrator", "AuthorizedUser", "JBossAdministrator" ], \
\n            "roleToAuthMapping": { \
\n                "PRIVATE": [ "PVT" ], \
\n                "PUBLIC": [ "PUB" ] \
\n            }, \
\n            "creationTime": -1, \
\n            "expirationTime": -1 \
\n        } \
\n        ]]></value> \
\n        <value><![CDATA[ \
\n        { \
\n            "dn": { \
\n                "subjectDN": "cn=testserver.example.com, ou=d009, o=my company, st=some-state, c=us", \
\n                "issuerDN": "cn=test ca, ou=my department, o=my company, st=some-state, c=us" \
\n            }, \
\n            "userType": "SERVER",\
\n            "auths": [ "PVT", "PUB" ], \
\n            "roles": [ "PRIVATE", "PUBLIC", "AuthorizedServer" ], \
\n            "roleToAuthMapping": { \
\n                "PRIVATE": [ "PVT" ], \
\n                "PUBLIC": [ "PUB" ] \
\n            }, \
\n            "creationTime": -1, \
\n            "expirationTime": -1 \
\n        } \
\n        ]]></value>

# other properties
rpm.file.owner=rpmowner
rpm.file.group=rpmowner
rpm.file.accumulo.owner=accumulo-owner
rpm.file.accumulo.group=accumulo-owner

# Enable full table scans for the base event query?
#beq.fullTableScanEnabled=true

event.query.data.decorators= \
          <entry key="CSV"> \
\n            <bean class="datawave.query.transformer.EventQueryDataDecorator"> \
\n                <property name="fieldName" value="CSV"/> \
\n                <property name="patternMap"> \
\n                    <map key-type="java.lang.String" value-type="java.lang.String"> \
\n                        <entry key="EVENT_ID" value="https://localhost:8443/DataWave/Query/lookupUUID/EVENT_ID?uuid=@field_value@&amp;parameters=data.decorators:CSV"/> \
\n                        <entry key="UUID" value="https://localhost:8443/DataWave/Query/lookupUUID/UUID?uuid=@field_value@&amp;parameters=data.decorators:CSV"/> \
\n                        <entry key="PARENT_UUID" value="https://localhost:8443/DataWave/Query/lookupUUID/PARENT_UUID?uuid=@field_value@&amp;parameters=data.decorators:CSV"/> \
\n                    </map> \
\n                </property> \
\n                <property name="responseObjectFactory" ref="responseObjectFactory" /> \
\n            </bean> \
\n        </entry> \
\n        <entry key="WIKIPEDIA"> \
\n            <bean class="datawave.query.transformer.EventQueryDataDecorator"> \
\n                <property name="fieldName" value="WIKIPEDIA"/> \
\n                <property name="patternMap"> \
\n                    <map key-type="java.lang.String" value-type="java.lang.String"> \
\n                        <entry key="PAGE_ID" value="https://localhost:8443/DataWave/Query/lookupUUID/PAGE_ID?uuid=@field_value@&amp;parameters=data.decorators:WIKIPEDIA"/> \
\n                        <entry key="PAGE_TITLE" value="https://localhost:8443/DataWave/Query/lookupUUID/PAGE_TITLE?uuid=@field_value@&amp;parameters=data.decorators:WIKIPEDIA"/> \
\n                    </map> \
\n                </property> \
\n                <property name="responseObjectFactory" ref="responseObjectFactory" /> \
\n            </bean> \
\n        </entry>

lookup.uuid.uuidTypes= \
          <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="ID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="EMBEDDED_CAST_PERSON_ID" /> \
\n            <property name="queryLogics">\
\n              <util:map>\
\n                <entry key="default" value="LuceneUUIDEventQuery"/> \
\n              </util:map> \
\n            </property> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="EVENT_ID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n            <property name="allowWildcardAfter" value="28" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="UUID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PARENT_UUID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PAGE_ID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PAGE_TITLE" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean>

query.metrics.marking=(PUBLIC)
query.metrics.visibility=PUBLIC

metrics.warehouse.namenode=localhost
metrics.warehouse.hadoop.path=/local/hadoop
metrics.reporter.class=datawave.metrics.NoOpMetricsReporterFactory

metadatahelper.default.auths=PUBLIC

security.npe.ou.entries=EXAMPLE_SERVER_OU1,EXAMPLE_SERVER_OU2
security.subject.dn.pattern=(?:^|,)\\s*OU\\s*=\\s*My Department\\s*(?:,|$)

datawave.docs.menu.extras=<li><a href="http://localhost:9995">Accumulo</a></li>
