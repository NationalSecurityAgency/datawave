CONFIGURATION=compose
RCPT_TO=hadoop@localhost

docker.image.prefix=ghcr.io/nationalsecurityagency/

docker.image.accumulo.tag=2.1.3

# ingest properties
DATAWAVE_INGEST_HOME=/opt/datawave-ingest/current

trusted.header.login=true

hdfs.site.config.urls=file:///etc/hadoop/conf/core-site.xml,file:///etc/hadoop/conf/hdfs-site.xml

WAREHOUSE_ACCUMULO_HOME=/opt/accumulo
WAREHOUSE_HDFS_NAME_NODE=hdfs://hdfs-nn:9000
WAREHOUSE_JOBTRACKER_NODE=yarn-rm:8032
WAREHOUSE_ZOOKEEPERS=zookeeper:2181
WAREHOUSE_INSTANCE_NAME=dev
#Sets variable sets the zookeeper location for the warehouse side
zookeeper.hosts=zookeeper:2181

INGEST_ACCUMULO_HOME=/opt/accumulo
INGEST_HDFS_NAME_NODE=hdfs://hdfs-nn:9000
INGEST_JOBTRACKER_NODE=yarn-rm:8032
INGEST_ZOOKEEPERS=zookeeper:2181
INGEST_INSTANCE_NAME=dev

JOB_CACHE_REPLICATION=1

STAGING_HOSTS=`nodeattr -c staging`
DUMPER_HOSTS=ingestmaster
INGEST_HOST=ingestmaster
ROLLUP_HOST=ingestmaster

#extra mapreduce options (e.g. mapreduce.task.io.sort.mb and the like)
MAPRED_INGEST_OPTS=-useInlineCombiner

#extra HADOOP_OPTS (java options)
HADOOP_INGEST_OPTS=

#extra CHILD_OPTS (java options)
CHILD_INGEST_OPTS=

BULK_CHILD_MAP_MAX_MEMORY_MB=2048
LIVE_CHILD_MAP_MAX_MEMORY_MB=1024
BULK_CHILD_REDUCE_MAX_MEMORY_MB=2048
LIVE_CHILD_REDUCE_MAX_MEMORY_MB=1024

BULK_INGEST_DATA_TYPES=shardStats
LIVE_INGEST_DATA_TYPES=wikipedia,mycsv,myjson

# Clear out these values if you do not want standard shard ingest.
DEFAULT_SHARD_HANDLER_CLASSES=datawave.ingest.mapreduce.handler.shard.AbstractColumnBasedHandler
ALL_HANDLER_CLASSES=datawave.ingest.mapreduce.handler.edge.ProtobufEdgeDataTypeHandler,datawave.ingest.mapreduce.handler.dateindex.DateIndexDataTypeHandler

BULK_INGEST_REDUCERS=10
LIVE_INGEST_REDUCERS=10

# Note the max blocks per job must be less than or equal to the number of mappers
INGEST_BULK_JOBS=1
INGEST_BULK_MAPPERS=4
INGEST_MAX_BULK_BLOCKS_PER_JOB=4
INGEST_LIVE_JOBS=1
INGEST_LIVE_MAPPERS=4
INGEST_MAX_LIVE_BLOCKS_PER_JOB=4

INDEX_STATS_MAX_MAPPERS=1

NUM_MAP_LOADERS=1

USERNAME=root
PASSWORD=root

ZOOKEEPER_HOME=/usr/lib/zookeeper
HADOOP_HOME=/usr/local/hadoop
MAPRED_HOME=/usr/local/hadoop-mapreduce

WAREHOUSE_HADOOP_CONF=/usr/local/hadoop/etc/hadoop/
INGEST_HADOOP_CONF=/usr/local/hadoop/etc/hadoop/

HDFS_BASE_DIR=/data

MONITOR_SERVER_HOST=monitor

LOG_DIR=/srv/logs/ingest
PDSH_LOG_DIR=${LOG_DIR}/pdsh_logs
FLAG_DIR=/srv/data/datawave/flags
FLAG_MAKER_CONFIG=/opt/datawave-ingest/current/config/flag-maker-live.xml,/opt/datawave-ingest/current/config/flag-maker-bulk.xml
BIN_DIR_FOR_FLAGS=/opt/datawave-ingest/current/bin

PYTHON=/usr/bin/python

# Setting discard interval to 0 in order to disable auto-ageoff @ ingest time
EVENT_DISCARD_INTERVAL=0

DATAWAVE_CACHE_PORT=20444

EDGE_DEFINITION_FILE=config/edge-definitions.xml

ERROR_TABLE=errors
ANALYTIC_MTX=analytic_metrics
LOADER_MTX=loader_metrics
INGEST_MTX=ingest_metrics
BULK_INGEST_METRIC_THRESHOLD=1500000
LIVE_INGEST_METRIC_THRESHOLD=1500000

KEYSTORE=/data/certs/keystore.p12
KEYSTORE_TYPE=PKCS12
KEYSTORE_PASSWORD=changeme
TRUSTSTORE=/data/certs/truststore.jks

FLAG_METRICS_DIR=/srv/data/datawave/flagMetrics
TRUSTSTORE_PASSWORD=changeme
TRUSTSTORE_TYPE=JKS

cluster.name=WAREHOUSE
accumulo.instance.name=dev
accumulo.user.name=root
accumulo.user.password=root
cached.results.hdfs.uri=hdfs://hdfs-nn:9000/
cached.results.export.dir=/CachedResults

lock.file.dir=/var/run/datawave
JAVA_HOME=/usr/lib/jvm/java/

# query properties
server.keystore.password=changeme
mysql.user.password=datawave
jboss.jmx.password=blah
hornetq.cluster.password=blah
hornetq.system.password=blah

server.truststore.password=changeme

#Sets up the Atom Service
atom.wildfly.hostname=localhost
atom.wildfly.port.number=8443
atom.connection.pool.name=WAREHOUSE

PASSWORD_INGEST_ENV=/opt/datawave-ingest/ingest-passwd.sh

security.use.testauthservice=false
security.testauthservice.context.entry=<value>classpath*:datawave/security/TestDatawaveUserServiceConfiguration.xml</value>

security.testauthservice.users= \
\n        <value><![CDATA[ \
\n        { \
\n            "dn": { \
\n                "subjectDN": "cn=testserver.example.com, ou=servers, o=example corp, c=us", \
\n                "issuerDN": "cn=example corp ca, o=example corp, c=us" \
\n            }, \
\n            "userType": "SERVER",\
\n            "auths": [ "PVT", "PUB", "PUBLIC","PRIVATE","FOO","BAR","DEF","A","B","C","D","E","F","G","H","I","DW_USER","DW_SERV","DW_ADMIN","JBOSS_ADMIN" ], \
\n            "roles": [ "PRIVATE", "PUBLIC", "AuthorizedServer" ], \
\n            "roleToAuthMapping": { \
\n                "PRIVATE": [ "PVT" ], \
\n                "PUBLIC": [ "PUB" ] \
\n            }, \
\n            "creationTime": -1, \
\n            "expirationTime": -1 \
\n        } \
\n        ]]></value>

security.use.remoteauthservice=true
security.remoteuserservice.scheme=https
# The host to connect to (or do a SRV lookup on) for the remote user service
security.remoteuserservice.host=dwv-web-authorization
# The port to connect to (unless a SRV lookup was performed) for the remote user service
security.remoteuserservice.port=8443

webapp.transport.guarantee=NONE

# other properties
rpm.file.owner=datawave
rpm.file.group=datawave
rpm.file.accumulo.owner=accumulo
rpm.file.accumulo.group=accumulo

# Enable full table scans for the base event query?
#beq.fullTableScanEnabled=true

event.query.data.decorators= \
          <entry key="CSV"> \
\n            <bean class="datawave.query.transformer.EventQueryDataDecorator"> \
\n                <property name="fieldName" value="CSV"/> \
\n                <property name="patternMap"> \
\n                    <map key-type="java.lang.String" value-type="java.lang.String"> \
\n                        <entry key="EVENT_ID" value="https://localhost:8443/DataWave/Query/lookupUUID/EVENT_ID?uuid=@field_value@&amp;parameters=data.decorators:CSV"/> \
\n                        <entry key="UUID" value="https://localhost:8443/DataWave/Query/lookupUUID/UUID?uuid=@field_value@&amp;parameters=data.decorators:CSV"/> \
\n                        <entry key="PARENT_UUID" value="https://localhost:8443/DataWave/Query/lookupUUID/PARENT_UUID?uuid=@field_value@&amp;parameters=data.decorators:CSV"/> \
\n                    </map> \
\n                </property> \
\n            </bean> \
\n        </entry> \
\n        <entry key="WIKIPEDIA"> \
\n            <bean class="datawave.query.transformer.EventQueryDataDecorator"> \
\n                <property name="fieldName" value="WIKIPEDIA"/> \
\n                <property name="patternMap"> \
\n                    <map key-type="java.lang.String" value-type="java.lang.String"> \
\n                        <entry key="PAGE_ID" value="https://localhost:8443/DataWave/Query/lookupUUID/PAGE_ID?uuid=@field_value@&amp;parameters=data.decorators:WIKIPEDIA"/> \
\n                        <entry key="PAGE_TITLE" value="https://localhost:8443/DataWave/Query/lookupUUID/PAGE_TITLE?uuid=@field_value@&amp;parameters=data.decorators:WIKIPEDIA"/> \
\n                    </map> \
\n                </property> \
\n            </bean> \
\n        </entry>


lookup.uuid.uuidTypes= \
          <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="ID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="EMBEDDED_CAST_PERSON_ID" /> \
\n            <property name="queryLogics">\
\n              <util:map>\
\n                <entry key="default" value="LuceneUUIDEventQuery"/> \
\n              </util:map> \
\n            </property> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="EVENT_ID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n            <property name="allowWildcardAfter" value="28" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="UUID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PARENT_UUID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PAGE_ID" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean> \
\n        <bean class="datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PAGE_TITLE" /> \
\n            <property name="queryLogics" ref="DefaultUUIDQueryLogics" /> \
\n        </bean>



query.metrics.marking=(PUBLIC)
query.metrics.visibility=PUBLIC

metrics.warehouse.namenode=localhost
metrics.warehouse.hadoop.path=/usr/local/hadoop
metrics.reporter.class=datawave.metrics.NoOpMetricsReporterFactory

metadatahelper.default.auths=PUBLIC

security.npe.ou.entries=EXAMPLE_SERVER_OU1,EXAMPLE_SERVER_OU2
security.subject.dn.pattern=(?:^|,)\\s*OU\\s*=\\s*My Department\\s*(?:,|$)

datawave.docs.menu.extras=<li><a href="http://localhost:9995">Accumulo</a></li>

