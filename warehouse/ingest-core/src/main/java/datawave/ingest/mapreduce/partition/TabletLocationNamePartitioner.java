package datawave.ingest.mapreduce.partition;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;
import java.util.SortedSet;
import java.util.TreeSet;

import org.apache.accumulo.core.data.Value;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Partitioner;
import org.apache.log4j.Logger;

import datawave.ingest.mapreduce.job.BulkIngestKey;
import datawave.ingest.mapreduce.job.SplitsFile;

/**
 * The TabletLocationHashPartitioner will look up the shard tablet servers, sort those tablet servers and then partition based on the index into that sorted
 * list.
 */
public class TabletLocationNamePartitioner extends Partitioner<BulkIngestKey,Value> implements Configurable, DelegatePartitioner {
    private static final Logger log = Logger.getLogger(TabletLocationNamePartitioner.class);
    private Configuration conf;
    private Map<String,Map<Text,Integer>> shardLocations;

    @Override
    public synchronized int getPartition(BulkIngestKey key, Value value, int numReduceTasks) {

        try {
            return getLocationPartition(key.getKey().getRow(), getShardLocations(key.getTableName().toString()), numReduceTasks);
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public Configuration getConf() {
        return conf;
    }

    @Override
    public void setConf(Configuration conf) {
        this.conf = conf;
    }

    // this is used by two schemes
    /**
     * Given a map of shard IDs to tablet server locations, this method determines a partition for a given key's shard ID. The goal is that we want to ensure
     * that all shard IDs served by a given tablet server get sent to the same reducer. To do this, we look up where the shard ID is supposed to be stored and
     * use a hash of that (modded by the number of reduces) to come up with the final allocation. This mapping needs to be computed at job startup and, so long
     * as no migration goes on during a job, will produce a single map file per tablet server. Note that it is also possible that we receive data for a day that
     * hasn't been loaded yet. In that case, we'll just hash the shard ID and send data to that reducer. This will spread out the data for a given day, but the
     * map files produced for it will not belong to any given tablet server. So, in the worst case, we have other older data when is already assigned to a
     * tablet server and new data which is not. In this case, we'd end up sending two map files to each tablet server. Of course, if tablets get moved around
     * between when the job starts and the map files are loaded, then we may end up sending multiple map files to each tablet server.
     *
     * @param shardId
     *            the shard id
     * @param shardHash
     *            the shard hash map
     * @param numReduceTasks
     *            the number of reducer tasks
     * @return the partition
     */
    private int getLocationPartition(Text shardId, Map<Text,Integer> shardHash, int numReduceTasks) {
        int partition = 0;
        Integer hash = shardHash.get(shardId);
        if (hash != null) {
            partition = (hash & Integer.MAX_VALUE) % numReduceTasks;
        } else {
            partition = (shardId.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
        }
        return partition;
    }

    /**
     * Sorts the tserver locations for all the shard ids. Each shardId is assigned a number which corresponds to where its tserver is found in that sorted list.
     * sort by most recent shard id, tserver
     *
     * suggested change: sort by most recent day's locations first, not by location id
     *
     * Read in the sequence file (that was created at job startup) that contains a list of shard IDs and the corresponding tablet server to which that shard is
     * assigned. The location is a number generated by adding the tuples of the location ip address and port.
     *
     * @param tableName
     *            the table name
     * @return a map of the shard locations
     * @throws IOException
     *             for read write issues
     */
    private Map<Text,Integer> getShardLocations(String tableName) throws IOException {
        if (this.shardLocations == null) {
            this.shardLocations = new HashMap<>();
        }

        if (null == this.shardLocations.get(tableName)) {
            Map<Text,String> shards = SplitsFile.getSplitsAndLocations(conf, tableName);

            // now sort the locations
            SortedSet<String> locations = new TreeSet<>();
            locations.addAll(shards.values());

            ArrayList<String> locList = new ArrayList<>(locations);
            HashMap<Text,Integer> localShardLocations = new HashMap<>();
            for (Map.Entry<Text,String> entry : shards.entrySet()) {
                localShardLocations.put(entry.getKey(), locList.indexOf(entry.getValue()));
            }

            this.shardLocations.put(tableName, localShardLocations);
        }

        return this.shardLocations.get(tableName);
    }

    @Override
    public void configureWithPrefix(String prefix) {/* no op */}

    @Override
    public int getNumPartitions() {
        return Integer.MAX_VALUE;
    }

    @Override
    public void initializeJob(Job job) {}
}
