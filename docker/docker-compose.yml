version: '2.2'
volumes:
  quickstart_data:
  hadoop_conf:

services:
  quickstart:
    profiles:
      - quickstart
    # To run the wildfly webservice, change `--accumulo` to `--web`
    command: ["datawave-bootstrap.sh", "--accumulo"]
    image: datawave/quickstart-compose
    environment:
      - DW_CONTAINER_HOST=quickstart
      - DW_DATAWAVE_WEB_JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:8787 -Duser.timezone=GMT -Dfile.encoding=UTF-8 -Djava.net.preferIPv4Stack=true
    ports:
      # resource manager web ui
      - "8088:8088"
      # resource manager
      - "8032:8032"
      # node manager web ui
      - "8042:8042"
      # namenode server
      - "9000:9000"
      # namenode web ui
      - "9870:9870"
      # datanode web ui
      - "9864:9864"
      # jobhistory web ui
      - "8021:8021"
      # accumulo monitor
      - "9995:9995"
      # web server
      - "9443:8443"
      # web server debug port
      - "5011:8787"
    extra_hosts:
      - "${DW_HOSTNAME}:${DW_HOST_IP}"
      - "${DW_HOST_FQDN}:${DW_HOST_IP}"
    volumes:
      - hadoop_conf:/opt/datawave/contrib/datawave-quickstart/hadoop/client/conf
      - quickstart_data:/opt/datawave/contrib/datawave-quickstart/data
      - ./logs:/logs
    networks:
      - demo
    healthcheck:
      test: ["CMD-SHELL", "! accumuloStatus | grep DW-WARN > /dev/null"]

  consul:
    image: docker.io/hashicorp/consul:1.15.4
    hostname: localhost
    environment:
      - 'CONSUL_LOCAL_CONFIG={"log_level": "trace", "datacenter": "demo_dc", "disable_update_check": true, "enable_agent_tls_for_checks": true, "addresses": {"https": "0.0.0.0"}, "ports": {"https": 8501, "grpc_tls": 8503}, "tls": {"defaults": {"key_file": "/etc/pki/testServer.key", "cert_file": "/etc/pki/testServer.crt", "ca_file": "/etc/pki/testCA.pem", "verify_outgoing": true}, "internal_rpc": {"verify_server_hostname": false}}}'
      - CONSUL_BIND_INTERFACE=eth0
    # defined as host:container
    ports:
      - "8400"
      - "8500:8500"
      - "8501:8501"
      - "8503:8503"
      - "53"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
    networks:
      - demo

  rabbitmq:
    image: docker.io/rabbitmq:3.12.4
    volumes:
      - ${RABBITMQ_CONFIG_DIR:-./rabbitmq-config}:/etc/rabbitmq
      - ./logs:/logs
    environment:
      - TCP_PORTS=15672, 5672
      - RABBITMQ_ERLANG_COOKIE="mycookie"
    ports:
      - "15672:15672"
    networks:
      - demo
    depends_on:
      consul:
        condition: service_started

  # When auto.create.topics.enable is true, this causes deleted topics to be recreated at random.  So, leave it disabled.
  kafka:
    profiles:
      - kafka
    image: docker.io/bitnami/kafka:3.2
    ports:
      - "9094:9094"
    networks:
      - demo
    environment:
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://${DW_HOSTNAME}:9094
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_INTER_BROKER_LISTENER_NAME=CLIENT
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=false
      - KAFKA_CFG_DELETE_TOPICS_ENABLE=true

  kafdrop:
    profiles:
      - kafka
    image: docker.io/obsidiandynamics/kafdrop
    ports:
      - "8999:9000"
    networks:
      - demo
    environment:
      - "KAFKA_BROKERCONNECT=${DW_HOSTNAME}:9094"
    # This mapping is required to enable kafdrop to communicate with
    # the external, host-bound port for kafka
    extra_hosts:
      - "${DW_HOSTNAME}:${DW_HOST_IP}"
      - "${DW_HOST_FQDN}:${DW_HOST_IP}"
    depends_on:
      kafka:
        condition: service_started

  configuration:
    entrypoint: [ "java","-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5009","-jar","app.jar" ]
    image: datawave/config-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,native,open_actuator
      - --spring.cloud.consul.host=consul
      - --spring.cloud.config.server.native.searchLocations=file:///microservice-config
    environment:
      - 'KEYSTORE_LOCATION=file:///etc/pki/testServer.p12'
      - KEYSTORE_PASSWORD=ChangeIt
      - KEY_ALIAS=certificate
    ports:
      - "8888:8888"
      - "5009:5009"
    volumes:
      - ${CONFIG_DIR:-./config}:/microservice-config:ro
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo
    depends_on:
      rabbitmq:
        condition: service_started

  cache:
    image: datawave/hazelcast-service
    scale: 1
    command:
      - --spring.profiles.active=consul,compose,remoteauth
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    ports:
      - "5701-5703"
      - "8080"
      - "8843:8443"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/cache/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 45s
      retries: 3
    depends_on:
      configuration:
        condition: service_started

  query-cache:
    profiles:
      - querycache
      # To enable an additional Hazelcast cluster, enable the "querycache" profile
      # Set an environment variable: QUERY_CACHE=query-cache. This will force the query and executor service to use a separate "query-cache" Hazelcast cluster.
    image: datawave/hazelcast-service
    scale: 1
    command:
      - --spring.application.name=query-cache
      - --hazelcast.client.cluster-members
      - --spring.profiles.active=consul,compose,remoteauth
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    ports:
      - "5701-5703"
      - "8080"
      - "8844:8443"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/query-cache/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 45s
      retries: 3
    depends_on:
      configuration:
        condition: service_started

  query-rabbitmq:
    profiles:
      - queryrabbit
      # To enable an additional rabbit cluster, enable the "queryrabbit" profile
      # Set an environment variable: USE_DEDICATED_INSTANCE=true. This will force the query and executor service to use a separate "query-rabbitmq" rabbit cluster.
    image: docker.io/rabbitmq:3.12.4
    volumes:
      - ${RABBITMQ_CONFIG_DIR:-./rabbitmq-query-config}:/etc/rabbitmq
      - ./logs:/logs
    environment:
      - TCP_PORTS=15672,5672
      - RABBITMQ_ERLANG_COOKIE="someothercookie"
    ports:
      - "15673:15672"
    networks:
      - demo
    depends_on:
      consul:
        condition: service_started

  # When auto.create.topics.enable is true, this causes deleted topics to be recreated at random.  So, leave it disabled.
  query-kafka:
    profiles:
      - querykafka
    # To enable an additional rabbit cluster, enable the "querykafka" profile
    # Set an environment variable: USE_DEDICATED_INSTANCE=true. This will force the query and executor service to use a separate "query-kafka" kafka cluster.
    image: docker.io/bitnami/kafka:3.2
    ports:
      - "9095:9095"
    networks:
      - demo
    environment:
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,CONTROLLER://:9093,EXTERNAL://:9095
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://query-kafka:9092,EXTERNAL://${DW_HOSTNAME}:9095
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@query-kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_INTER_BROKER_LISTENER_NAME=CLIENT
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=false
      - KAFKA_CFG_DELETE_TOPICS_ENABLE=true

  authorization:
    entrypoint: [ "java","-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5008","-jar","app.jar" ]
    image: datawave/authorization-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,mock,compose,federation
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    ports:
      - "8080"
      - "8343:8443"
      - "5008:5008"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/authorization/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 20s
      retries: 3
    depends_on:
      cache:
        condition: service_healthy

  accumulo:
    profiles:
      - accumulo
      - full
    image: datawave/accumulo-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    environment:
      - AUDIT_SERVER_URL=http://audit:8080/audit
      - ZOOKEEPER_HOST=${DW_ZOOKEEPER_HOST}
    # This mapping is required to enable the metrics service to communicate
    # with host-deployed services like hadoop, zookeeper, and accumulo.
    # These values are set locally in .env via bootstrap.sh
    extra_hosts:
      - "${DW_HOSTNAME}:${DW_HOST_IP}"
      - "${DW_HOST_FQDN}:${DW_HOST_IP}"
    ports:
      - "9143:8443"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/accumulo/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 45s
      retries: 3
    depends_on:
      authorization:
        condition: service_healthy

  audit:
    image: datawave/audit-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    environment:
      - ZOOKEEPER_HOST=${DW_ZOOKEEPER_HOST}
    # This mapping is required to enable the audit service to communicate
    # with host-deployed services like hadoop, zookeeper, and accumulo.
    # These values are set locally in .env via bootstrap.sh
    extra_hosts:
      - "${DW_HOSTNAME}:${DW_HOST_IP}"
      - "${DW_HOST_FQDN}:${DW_HOST_IP}"
    ports:
      - "8080"
      - "9043:8443"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/audit/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 30s
      retries: 3
    depends_on:
      authorization:
        condition: service_healthy

  metrics:
    entrypoint: ["java","-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5007","-jar","app.jar"]
    image: datawave/query-metric-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    environment:
      - ZOOKEEPER_HOST=${DW_ZOOKEEPER_HOST}
    # This mapping is required to enable the metrics service to communicate
    # with host-deployed services like hadoop, zookeeper, and accumulo.
    # These values are set locally in .env via bootstrap.sh
    extra_hosts:
      - "${DW_HOSTNAME}:${DW_HOST_IP}"
      - "${DW_HOST_FQDN}:${DW_HOST_IP}"
    ports:
      - "8180:8080"
      - "8543:8443"
      - "5007:5007"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/querymetric/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 45s
      retries: 3
    depends_on:
      authorization:
        condition: service_healthy

  dictionary:
    profiles:
      - dictionary
      - full
    image: datawave/dictionary-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    environment:
      - ZOOKEEPER_HOST=${DW_ZOOKEEPER_HOST}
    # This mapping is required to enable the metrics service to communicate
    # with host-deployed services like hadoop, zookeeper, and accumulo.
    # These values are set locally in .env via bootstrap.sh
    extra_hosts:
      - "${DW_HOSTNAME}:${DW_HOST_IP}"
      - "${DW_HOST_FQDN}:${DW_HOST_IP}"
    ports:
      - "8280:8080"
      - "8643:8443"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/dictionary/mgmt/health
    depends_on:
      authorization:
        condition: service_healthy

  file-provider:
    profiles:
      - file-provider
      - full
    image: datawave/file-provider-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    ports:
      - "8280:8080"
      - "8643:8443"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo

  # If you want to test cached results, enable the cachedresults profile
  mysql:
    profiles:
      - cachedresults
    image: docker.io/mysql:8.0.32
    environment:
      - MYSQL_RANDOM_ROOT_PASSWORD=true
      - MYSQL_DATABASE=cachedresults
      - MYSQL_USER=datawave
      - MYSQL_PASSWORD=secret
    networks:
      - demo
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      timeout: 20s
      retries: 10

  # If you want to test cached results, set the CACHED_RESULTS environment variable to 'true'
  query:
    entrypoint: ["java","-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005","-jar","app.jar"]
    image: datawave/query-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth,querymessaging,metricssource,query,mrquery,cachedresults,federation
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    environment:
      - AUDIT_SERVER_URL=http://audit:8080/audit
      - HADOOP_HOST=${DW_HADOOP_HOST}
      - HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/etc/hadoop/conf}
      - "BACKEND=${BACKEND:-rabbitmq}"
      - CACHED_RESULTS=${CACHED_RESULTS:-false}
      - QUERY_CACHE=${QUERY_CACHE:-cache}
      - USE_DEDICATED_INSTANCE=${USE_DEDICATED_INSTANCE:-false}
    ports:
      - "8080:8080"
      - "8443:8443"
      - "5005:5005"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
      - ${HADOOP_CONF_DIR:-hadoop_conf}:${HADOOP_CONF_DIR:-/etc/hadoop/conf}:ro
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/query/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 30s
      retries: 3
    depends_on:
      audit:
        condition: service_healthy
      authorization:
        condition: service_healthy
      metrics:
        condition: service_healthy
      executor-pool1:
        condition: service_started

  mapreduce-query:
    profiles:
      - full
    entrypoint: ["java","-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005","-jar","app.jar"]
    image: datawave/mapreduce-query-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth,query,mrquery,federation
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    environment:
      - ZOOKEEPER_HOST=${DW_ZOOKEEPER_HOST}
      - HADOOP_HOST=${DW_HADOOP_HOST}
      - HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/etc/hadoop/conf}
    ports:
      - "50005:5005"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
      - ${HADOOP_CONF_DIR:-hadoop_conf}:${HADOOP_CONF_DIR:-/etc/hadoop/conf}:ro
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/mrquery/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 30s
      retries: 3
    depends_on:
      audit:
        condition: service_healthy
      authorization:
        condition: service_healthy
      metrics:
        condition: service_healthy
      executor-pool1:
        condition: service_started

  executor-pool1:
    entrypoint: ["java","-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5006","-jar","app.jar"]
    image: datawave/query-executor-service
    command:
      - --spring.application.name=executor-pool1
      - --spring.cloud.config.name=executor
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth,querymessaging,metricssource,query,pool1,federation
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    environment:
      - ZOOKEEPER_HOST=${DW_ZOOKEEPER_HOST}
      - HADOOP_HOST=${DW_HADOOP_HOST}
      - BACKEND=${BACKEND:-rabbitmq}
      - HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/etc/hadoop/conf}
      - QUERY_CACHE=${QUERY_CACHE:-cache}
      - USE_DEDICATED_INSTANCE=${USE_DEDICATED_INSTANCE:-false}
    # This mapping is required to enable the metrics service to communicate
    # with host-deployed services like hadoop, zookeeper, and accumulo.
    # These values are set locally in .env via bootstrap.sh
    extra_hosts:
      - "${DW_HOSTNAME}:${DW_HOST_IP}"
      - "${DW_HOST_FQDN}:${DW_HOST_IP}"
    ports:
      - "8380:8080"
      - "8743:8443"
      - "5006:5006"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs/pool1:/logs
      - ${HADOOP_CONF_DIR:-hadoop_conf}:${HADOOP_CONF_DIR:-/etc/hadoop/conf}:ro
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/executor/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 30s
      retries: 3
    depends_on:
      rabbitmq:
        condition: service_started
      authorization:
        condition: service_healthy
      metrics:
        condition: service_healthy

  executor-pool2:
    profiles:
      - pool2
      - full
    image: datawave/query-executor-service
    command:
      - --spring.application.name=executor-pool2
      - --spring.cloud.config.name=executor
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth,querymessaging,metricssource,query,pool2,federation
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    environment:
      - ZOOKEEPER_HOST=${DW_ZOOKEEPER_HOST}
      - HADOOP_HOST=${DW_HADOOP_HOST}
      - BACKEND=${BACKEND:-rabbitmq}
      - HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/etc/hadoop/conf}
      - QUERY_CACHE=${QUERY_CACHE:-cache}
      - USE_DEDICATED_INSTANCE=${USE_DEDICATED_INSTANCE:-false}
    # This mapping is required to enable the metrics service to communicate
    # with host-deployed services like hadoop, zookeeper, and accumulo.
    # These values are set locally in .env via bootstrap.sh
    extra_hosts:
      - "${DW_HOSTNAME}:${DW_HOST_IP}"
      - "${DW_HOST_FQDN}:${DW_HOST_IP}"
    ports:
      - "8480:8080"
      - "8243:8443"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs/pool2:/logs
      - ${HADOOP_CONF_DIR:-hadoop_conf}:${HADOOP_CONF_DIR:-/etc/hadoop/conf}:ro
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/executor/mgmt/health
      interval: 10s
      timeout: 1s
      start_period: 30s
      retries: 3
    depends_on:
      rabbitmq:
        condition: service_started
      authorization:
        condition: service_healthy
      metrics:
        condition: service_healthy

  modification:
    entrypoint: ["java","-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5010","-jar","app.jar"]
    profiles:
      - modification
      - full
    image: datawave/modification-service
    command:
      - --spring.output.ansi.enabled=ALWAYS
      - --spring.profiles.active=consul,compose,remoteauth,query
      - --spring.cloud.consul.host=consul
      - --spring.cloud.consul.discovery.instance-id=$${spring.application.name}:$${random.value}
    environment:
      - ZOOKEEPER_HOST=${DW_ZOOKEEPER_HOST}
    # This mapping is required to enable the metrics service to communicate
    # with host-deployed services like hadoop, zookeeper, and accumulo.
    # These values are set locally in .env via bootstrap.sh
    extra_hosts:
      - "${DW_HOSTNAME}:${DW_HOST_IP}"
      - "${DW_HOST_FQDN}:${DW_HOST_IP}"
    ports:
      - "8680:8080"
      - "9343:8443"
      - "5010:5010"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
      - ./logs:/logs
    networks:
      - demo
    healthcheck:
      test: curl -f http://localhost:8080/dictionary/mgmt/health
    depends_on:
      authorization:
        condition: service_healthy

  # If you use the management center, you can connect to the hazelcast cache as follows:
  # In your browser connect to https://localhost:9243/
  # Enable 'dev' mode
  # Click 'Add Cluster Config'
  # Enter the following for the cache service:
  #  - Cluster Name: cache
  #  - Cluster Config: Enabled
  #  - Member Addresses: cache
  # Enter the following for the query metric service:
  #  - Cluster Name: metrics
  #  - Cluster Config: Enabled
  #  - Member Addresses: metrics
  # Use the console to view the cache contents
  #  - Select the 'cache' cluster
  #  - Select 'Console' under the 'CLUSTER' navigation entry
  #  - Run the following commands to list all entries in the 'datawaveUsers' map:
  #    - ns datawaveUsers
  #    - m.entries
  management-center:
    profiles:
      - management
      - full
    image: docker.io/hazelcast/management-center:5.1.2
    environment:
      - |-
        JAVA_OPTS=
        -Dhazelcast.mc.healthCheck.enable=true
        -Dhazelcast.mc.tls.enabled=true
        -Dhazelcast.mc.tls.keyStore=/etc/pki/testServer.p12
        -Dhazelcast.mc.tls.keyStorePassword=ChangeIt
        -Dhazelcast.mc.tls.trustStore=/etc/pki/testCA.p12
        -Dhazelcast.mc.tls.trustStorePassword=ChangeIt
    ports:
      - "8081"
      - "9243:8443"
    volumes:
      - ${PKI_DIR:-./pki}:/etc/pki:ro
    networks:
      - demo
    healthcheck:
      test: wget -q http://localhost:8081/health -O /dev/null || exit 1
    depends_on:
      cache:
        condition: service_healthy

networks:
  demo:
